{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195014ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "GROQ_API_KEY = os.getenv('groq_key')\n",
    "\n",
    "FILE_PATH = r'C:\\Users\\fuedj\\Documents\\Code\\RAG_Dr_Voss\\llm-case-study-main\\src\\parte1_another.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2942147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(file_path: str) -> str:\n",
    "    \"\"\"Carrega o conteúdo do arquivo de texto\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 800) -> List[str]:\n",
    "    \"\"\"Divide o texto em chunks\"\"\"\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "text = load_txt(FILE_PATH)\n",
    "\n",
    "chunks = chunk_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_chunk(chunk: str, max_size: int = 8000) -> list[str]:\n",
    "    parts = []\n",
    "    while len(chunk) > max_size:\n",
    "        cut_index = chunk.rfind('.', 0, max_size)\n",
    "        if cut_index == -1:\n",
    "            cut_index = chunk.rfind(' ', 0, max_size)\n",
    "        if cut_index == -1:\n",
    "            cut_index = max_size  # Não achou ponto nem espaço, corta bruto\n",
    "        parts.append(chunk[:cut_index+1].strip())\n",
    "        chunk = chunk[cut_index+1:].strip()\n",
    "    if chunk:\n",
    "        parts.append(chunk)\n",
    "    return parts\n",
    "\n",
    "def chunk_diary_by_day_and_paragraph(diary_text: str) -> list[str]:\n",
    "    chunks = []\n",
    "    current_day_content = \"\"\n",
    "    date_pattern = r\"(\\d{1,2})(?:st|nd|rd|th)? Day of ([A-Za-z]+) (18\\d{2}) - ([A-Za-z\\s]+)\"\n",
    "\n",
    "    for line in diary_text.splitlines():\n",
    "        if re.match(date_pattern, line):\n",
    "            # Encontrou um novo dia\n",
    "            if current_day_content:\n",
    "                # Divida o conteúdo do dia anterior por parágrafos (linhas não vazias)\n",
    "                paragraphs = [p.strip() for p in current_day_content.strip().split('\\n\\n') if p.strip()]\n",
    "                chunks.extend(paragraphs)\n",
    "            current_day_content = line + \"\\n\"  # Comece o conteúdo do novo dia\n",
    "        else:\n",
    "            current_day_content += line + \"\\n\"\n",
    "\n",
    "    # Processar o conteúdo do último dia\n",
    "    if current_day_content:\n",
    "        paragraphs = [p.strip() for p in current_day_content.strip().split('\\n\\n') if p.strip()]\n",
    "        for paragraph in paragraphs:\n",
    "            if len(paragraph) > 800:\n",
    "                chunks.extend(split_large_chunk(paragraph))\n",
    "            else:\n",
    "                chunks.append(paragraph)\n",
    "    \n",
    "    return [chunk for chunk in chunks if chunk] # Remove empty ones \n",
    "\n",
    "list_chunk = chunk_diary_by_day_and_paragraph(text)\n",
    "\n",
    "chunks = list_chunk\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "class SmartAgentSystem:\n",
    "    def __init__(self, api_key: str, question: str, all_chunks: List[str]):\n",
    "        self.api_key = api_key\n",
    "        self.question = question\n",
    "        self.all_chunks = all_chunks\n",
    "        self.processed_chunks = set()\n",
    "        self.partial_answers = []\n",
    "        self.session = None\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        self.session = aiohttp.ClientSession()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, *args):\n",
    "        await self.session.close()\n",
    "\n",
    "    async def expert_agent(self, chunk: str, chunk_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Agente especialista com identificação de chunk\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Analyze this text regarding: {self.question}\n",
    "            \n",
    "            Required JSON format:\n",
    "            {{\n",
    "                \"analysis\": {{\n",
    "                    \"key_findings\": [\"list\", \"of\", \"findings\"],\n",
    "                    \"missing_info\": [\"list\", \"of\", \"gaps\"]\n",
    "                }},\n",
    "                \"metadata\": {{\n",
    "                    \"chunk_id\": {chunk_id},\n",
    "                    \"relevance\": 0.0-1.0,\n",
    "                    \"needs_more_context\": true/false\n",
    "                }}\n",
    "            }}\n",
    "\n",
    "            Text: {chunk}\"\"\"\n",
    "\n",
    "            async with self.session.post(\n",
    "                \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n",
    "                json={\n",
    "                    \"model\": \"qwen-qwq-32b\",\n",
    "                    \"messages\": [{\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You must respond in valid JSON format exactly as specified.\"\n",
    "                    }, {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }],\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"response_format\": {\"type\": \"json_object\"}\n",
    "                },\n",
    "                timeout=20\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                data = await response.json()\n",
    "                result = json.loads(data['choices'][0]['message']['content'])\n",
    "                \n",
    "                print(f\"Expert Agent Decision: {result}\")\n",
    "                \n",
    "                # Validate response structure\n",
    "                required_keys = {'analysis', 'metadata'}\n",
    "                if not all(key in result for key in required_keys):\n",
    "                    raise ValueError(\"Missing required keys in response\")\n",
    "                \n",
    "                self.processed_chunks.add(chunk_id)\n",
    "                if result['metadata']['relevance'] > 0.4:\n",
    "                    self.partial_answers.append(result)\n",
    "                return result\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON Error in chunk {chunk_id}: {str(e)}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in chunk {chunk_id} response: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {chunk_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    async def central_agent(self) -> Dict:\n",
    "        \"\"\"Agente central que pode solicitar mais chunks\"\"\"\n",
    "        max_attempts = 1\n",
    "        print(f\"Partial Answers: {self.partial_answers}\")\n",
    "    \n",
    "        try:\n",
    "            context = \"\\n\".join(\n",
    "                f\"Chunk {ans['metadata']['chunk_id']}:\\n\"\n",
    "                f\"Findings: {ans['analysis']['key_findings']}\\n\"\n",
    "                f\"Missing: {ans['analysis']['missing_info']}\"\n",
    "                for ans in self.partial_answers\n",
    "            )\n",
    "            # context = \"\\n\".join(\n",
    "            #     f\"Chunk {ans['metadata']['chunk_id']}:\\n\"\n",
    "            #     f\"Findings: {ans['analysis']['key_findings']}\"\n",
    "                \n",
    "            #     for ans in self.partial_answers\n",
    "            # )\n",
    "\n",
    "            prompt = f\"\"\"Evaluate these analyses about: {self.question}\n",
    "            \n",
    "            Current Analysis:\n",
    "            {context}\n",
    "            \n",
    "            Required JSON response:\n",
    "            {{\n",
    "                \"status\": \"complete\" or \"incomplete\",\n",
    "                \"answer\": \"summary text\" or null,\n",
    "                \"requested_chunks\": [list, of, ids] or null,\n",
    "                \"confidence\": 0.0-1.0\n",
    "            }}\"\"\"\n",
    "\n",
    "            async with self.session.post(\n",
    "                \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n",
    "                json={\n",
    "                    \"model\": \"qwen-qwq-32b\",\n",
    "                    \"messages\": [{\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You must provide valid JSON. Double-check your output before returning.\"\n",
    "                    }, {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }],\n",
    "                    \"temperature\": 0.2,\n",
    "                    \"response_format\": {\"type\": \"json_object\"}\n",
    "                },\n",
    "                timeout=25\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                data = await response.json()\n",
    "                decision = json.loads(data['choices'][0]['message']['content'])\n",
    "                \n",
    "                print(f\"Central Agent Decision: {decision}\")\n",
    "                \n",
    "                # Validate central agent response\n",
    "                if not all(k in decision for k in [\"status\", \"answer\", \"requested_chunks\", \"confidence\"]):\n",
    "                    raise ValueError(\"Invalid central agent response format\")\n",
    "                \n",
    "                if decision[\"status\"] == \"complete\" or decision['confidence'] > 0.6:\n",
    "                    return {\n",
    "                        \"status\": \"success\",\n",
    "                        \"answer\": decision[\"answer\"],\n",
    "                        \"confidence\": decision[\"confidence\"],\n",
    "                        \"used_chunks\": [ans['metadata']['chunk_id'] for ans in self.partial_answers]\n",
    "                    }\n",
    "                \n",
    "                else:\n",
    "                    await self.process_additional_chunks(decision[\"requested_chunks\"])\n",
    "                    \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON Error: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Central agent error: {str(e)}\")\n",
    "        \n",
    "\n",
    "    async def process_additional_chunks(self, chunk_ids: List[int], first=False):\n",
    "        \"\"\"Processa chunks adicionais de forma simples e eficiente\"\"\"\n",
    "        # Filtra apenas chunks não processados e válidos\n",
    "        \n",
    "        chunks_to_process = self.all_chunks[:2]\n",
    "        \n",
    "        # Processa todos os chunks válidos de uma vez\n",
    "        if chunks_to_process:\n",
    "            print(f\"\\nChunks to Process: {len(chunks_to_process)}\")\n",
    "            print(\"Parte de um Chunk:\")\n",
    "            _ = [chunk[:30] for chunk_id, chunk in enumerate(chunks_to_process)]\n",
    "            print(_[:30])\n",
    "            tasks = [\n",
    "                self.expert_agent(chunk, chunk_id) \n",
    "                for chunk_id, chunk in enumerate(chunks_to_process)\n",
    "            ]\n",
    "            self.all_chunks = self.all_chunks[-1:]\n",
    "            print(f\"All Chunks: {self.all_chunks[:50]}\")\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "async def analyze_with_feedback(api_key: str, question: str, chunks: List[str]):\n",
    "    \"\"\"Versão simplificada do pipeline de análise\"\"\"\n",
    "    async with SmartAgentSystem(api_key, question, chunks) as system:\n",
    "        # Processa os 3 primeiros chunks inicialmente\n",
    "        await system.process_additional_chunks([0, 1, 2], first=True)\n",
    "        \n",
    "        # Obtém a decisão do agente central\n",
    "        \n",
    "        result = await system.central_agent()\n",
    "        \n",
    "        while not result:\n",
    "            result = await system.central_agent()\n",
    "        \n",
    "        print(f\"Result: {result}\")\n",
    "        \n",
    "        # Retorna o resultado seja qual for o status\n",
    "        return {\n",
    "            \"status\": result[\"status\"],\n",
    "            \"answer\": result.get(\"answer\"),\n",
    "            \"used_chunks\": list(system.processed_chunks)\n",
    "        }\n",
    "        \n",
    "\n",
    "# Exemplo de uso:\n",
    "async def main():\n",
    "    question = \"\"\"\n",
    "    Based on the diary entries, describe the uses of moonpetal by the people of Oakhaven and what the prevalence of this flower suggests about the local culture and economy.\n",
    "    \"\"\"\n",
    "    result = await analyze_with_feedback(GROQ_API_KEY, question, chunks)\n",
    "    \n",
    "    print(\"\\n=== Resultado Final ===\")\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Executar no Jupyter:\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
